{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\npaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def prepare_dataset(paths):\n    paths=paths[1:]\n    labels = pd.read_csv('/kaggle/input/words-label/words.txt',sep='\\n')[15:]\n\n    idd=labels['#--- words.txt ---------------------------------------------------------------#'].apply(lambda x:(x.split()[0])).values\n    lab = labels['#--- words.txt ---------------------------------------------------------------#'].apply(lambda x:(x.split()[-1]).lower()).values\n    targets = dict(zip(idd,lab))\n    \n    characters = set((''.join(lab)).lower())\n    word_ind = dict(zip(['<START>','<END>','<PAD>']+list(characters),range(len(characters)+3)))\n    \n    labell = [x.split('/')[-1][:-4] for x in paths]    \n    return dict(zip(labell,paths)),targets,list(targets.keys())[1:],word_ind","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import time\nimport os\nimport scipy \nimport scipy.io\nimport sys\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageDraw, ImageFile\n# fix bugs with loading png files\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nimport matplotlib.pyplot as plt\nimport PIL\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom torchvision import datasets, models, transforms\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset, Subset, random_split\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CaltechDataset(Dataset):\n    def __init__(self, imgs,dict_labels,name_ind,word_ind,transforms=None):\n        # path to the dataset\n        self.dict_labels=dict_labels\n        # transformation for image and targets\n        self.transforms = transforms\n        # load all image files, sorting them to\n        # ensure that they are aligned\n        self.imgs = imgs\n        self.name_ind=name_ind\n        self.word_ind = word_ind\n    def __getitem__(self, idx):\n        if self.name_ind[idx] not in self.imgs or self.name_ind[idx] not in self.dict_labels :\n            print(self.name_ind[idx])\n        img = cv2.imread(self.imgs[self.name_ind[idx]])\n        if img is None:\n            img = np.zeros((125,250,3))\n            print('bad img')\n        else:\n            img=img[:,:,::-1]\n        targ = self.dict_labels[self.name_ind[idx]]\n        \n        res = []#[self.word_ind['<START>']]\n        for x in targ:\n            res.append(self.word_ind[x])\n        res.append(self.word_ind['<END>'])\n        sent_len = len(res)\n        res+=[self.word_ind['<PAD>']]*(60-len(res))\n        if self.transforms is not None:\n            img= self.transforms(PIL.Image.fromarray(np.uint8(img)))\n        return img, torch.tensor(res),targ,sent_len\n\n    def __len__(self):\n        '''this method return len of dataset'''\n        return len(self.name_ind)","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.__len__()","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"115320"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"400*64","execution_count":30,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"25600"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"size = (125,250)\ntrans=transforms.Compose([\n            transforms.Resize(size),\n            transforms.ToTensor() # converts to [0,1] interval\n])\n\nimgs,dict_labels,name_ind,word_ind = prepare_dataset(paths)\n\ndf = CaltechDataset(imgs,dict_labels,name_ind,word_ind,transforms=trans)\n","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i,x in enumerate(df):\n    if i==2:\n        plt.imshow(x[0].permute(1,2,0))\n        print(x[1][:x[3]],x[2],x[3])\n        break   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv0 = nn.Conv2d(3,64,5)\n        self.conv1 = nn.Conv2d(64,64,3,padding=1)\n        self.conv2 = nn.Conv2d(64,64,3,padding=1)\n        self.conv3 = nn.Conv2d(64,64,3,padding=1)\n        self.conv4 = nn.Conv2d(128,128,3,padding=1)\n        self.conv5 = nn.Conv2d(128,128,3,padding=1)\n        self.conv6 = nn.Conv2d(128,128,3,padding=1)\n        self.conv7 = nn.Conv2d(256,256,3,padding=1)\n        self.batch1 = nn.BatchNorm2d(64)\n        self.batch2 = nn.BatchNorm2d(64)\n        self.batch3 = nn.BatchNorm2d(64)\n        self.batch4 = nn.BatchNorm2d(128)\n        self.batch5 = nn.BatchNorm2d(128)\n        self.batch6 = nn.BatchNorm2d(128)\n\n        \n        #-------------------\n        self.init_h =nn.Linear(30*61,256)\n        self.init_c=nn.Linear(30*61,256)\n        self.before_lstm = nn.Linear(30*61*4,64)\n        self.LSTM = nn.LSTM(64,256)\n        self.output_layer = nn.Linear(256,55)\n    def base(self,x):\n        x0=F.max_pool2d(F.relu(self.conv0(x)),2)\n        x1=self.batch1(F.relu(self.conv1(x0)))\n        x2=self.batch2(F.relu(self.conv2(x1)))\n        x3=self.batch3(F.relu(self.conv3(x2)))\n        x4=F.max_pool2d(self.batch4(F.relu(self.conv4(torch.cat((x3,x1),dim=1)))),2)\n        x5=self.batch5(F.relu(self.conv5(x4)))\n        x6=self.batch6(F.relu(self.conv6(x5)))\n        x7=F.relu(self.conv7(torch.cat((x6,x4),dim=1)))\n        return x7\n    def init_hidden(self,base_out):\n        hidden = base_out.mean(dim=1).view(-1,30*61)\n        h = self.init_h(hidden)  # (batch_size, decoder_dim)\n        c = self.init_c(hidden)\n        return h.unsqueeze(0), c.unsqueeze(0)\n    def forward(self,x,chars,length):\n        base_out=self.base(x)\n        hidden = self.init_hidden(base_out)\n        in_lstm=self.before_lstm(base_out.view(base_out.size(0),64,-1))\n        hidden = self.LSTM(in_lstm.permute(1,0,2),hidden)\n        out = F.log_softmax(self.output_layer(hidden[0]),dim=2)\n        \n        return out    ","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loader = DataLoader(df, batch_size=4,drop_last=True)\nmodel = Net()\nfor x,y,w,e in loader:\n    break","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"e","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"tensor([2, 5, 3, 5])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss = torch.nn.CTCLoss()\nloss(model(x,'',''),y, torch.full(size=(4,), fill_value=64, dtype=torch.long),e)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"tensor(73.4230, grad_fn=<MeanBackward0>)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nloader = DataLoader(df, batch_size=64,drop_last=True,shuffle=False)\nepochs = 20\n\ndevice = torch.device(\"cuda\")\nmodel = Net()\n# state = torch.load('loc.pth')\n# model.load_state_dict(state)\nmodel = model.to(device)\n# we optimizing parameters of our model, other parameters will not be optimised if exist  \noptimizer = optim.Adam(model.parameters(), lr=1e-4)\nloss_ctc = torch.nn.CTCLoss()","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model=model.to(device)","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.train()\nfor e in range(epochs):\n    t = 0\n    loss_avg = 0\n    \n    for i, (img, target,word,lns) in enumerate(loader):\n        img = img.to(device)\n        target = target.to(device)\n        lengths = lns.to(device)\n        optimizer.zero_grad()\n        #print(target_packed.data.size())\n        outputs= model(img,target,2)    \n        loss = loss_ctc(outputs,target, torch.full(size=(64,), fill_value=64, dtype=torch.long),lengths)   \n        # calculating gradients\n        loss.backward()\n        # optimizing\n        optimizer.step()\n        # .item() is simple way to get value from scalar\n        loss_avg += loss.item()\n        t+=1\n        if i%200==199:\n            print('LOSS',loss_avg/(t), e,i,word[0])\n            loss_avg=0\n            t=0\n            break","execution_count":43,"outputs":[{"output_type":"stream","text":"bad img\nLOSS 1.3779514861106872 0 199 in\nbad img\nLOSS 1.332351380586624 1 199 in\nbad img\nLOSS 1.2943692576885224 2 199 in\nbad img\nLOSS 1.2530533784627915 3 199 in\nbad img\nLOSS 1.213676399886608 4 199 in\nbad img\nLOSS 1.1733385920524597 5 199 in\nbad img\n","name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-5335aa81a597>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-25-d0d7d151a48b>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msent_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box)\u001b[0m\n\u001b[1;32m   1804\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1806\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1808\u001b[0m     def rotate(self, angle, resample=NEAREST, expand=0, center=None,\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.to(torch.device('cpu'))\nw=model(img.cpu(),torch.tensor([0]),10)","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"target[:10,:10]","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"tensor([[32, 42, 39,  1,  2,  2,  2,  2,  2,  2],\n        [11, 47, 49, 39, 21, 37, 40, 39, 37, 32],\n        [38, 47, 22, 41, 25,  1,  2,  2,  2,  2],\n        [37, 47, 32,  1,  2,  2,  2,  2,  2,  2],\n        [14, 39,  1,  2,  2,  2,  2,  2,  2,  2],\n        [ 5, 33, 21, 32, 16,  1,  2,  2,  2,  2],\n        [32, 47,  1,  2,  2,  2,  2,  2,  2,  2],\n        [33, 38, 38, 39,  5, 32, 12, 37, 11,  1],\n        [33, 23,  1,  2,  2,  2,  2,  2,  2,  2],\n        [33,  1,  2,  2,  2,  2,  2,  2,  2,  2]], device='cuda:0')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n    qe=w[:,i,:].exp().argmax(dim=1)\n    print(qe[qe!=0])","execution_count":52,"outputs":[{"output_type":"stream","text":"tensor([32, 42, 42, 39, 39,  1,  1])\ntensor([11, 47, 39, 40, 37, 40, 39, 32, 32,  1,  1])\ntensor([38, 37, 37, 41, 25,  1,  1])\ntensor([37, 47, 32, 32,  1,  1])\ntensor([14, 39, 39,  1,  1])\ntensor([ 5, 33, 33, 16,  1,  1])\ntensor([32, 47,  1,  1])\ntensor([33, 39, 32, 32, 12, 37, 11,  1,  1])\ntensor([33, 23,  1,  1])\ntensor([33,  1,  1])\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}